{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPING DATA OF REVIEWS FROM AMAZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating python program to search desired keywords one by one and saving links in open list prod_url\n",
    "\n",
    "# creating open list to save href links of all products \n",
    "prod_url = []\n",
    "\n",
    "# Keywords for search to which we need to scrape the data.\n",
    "keywords = ['Laptops', 'Phones', 'Headphones', 'Smart Watches', 'Cameras', 'Printers', 'Monitors','Home theater', 'Router']\n",
    "\n",
    "# Location of chrome web driver and launching it\n",
    "driver = webdriver.Chrome(\"C:/Flip Robo/Ratings Project/chromedriver.exe\") \n",
    "\n",
    "# website from which we will scrape data\n",
    "driver.get(\"https://www.amazon.in/\") \n",
    "\n",
    "initial_url = driver.current_url\n",
    "\n",
    "# loop for sending keywords to website and scraping href\n",
    "for i in keywords:\n",
    "    # Finding search input and giving input\n",
    "    search = driver.find_element_by_id(\"twotabsearchtextbox\").send_keys(i)\n",
    "        \n",
    "    # clicking on search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "    search_btn.click()\n",
    "    # giving time to load the webpage completely to avoid errors\n",
    "    time.sleep(5)\n",
    "    # finding href of each products mentioned in keywords\n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class = 'a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        prod_url.append(j.get_attribute('href'))\n",
    "    # returning to initial url    \n",
    "    driver.get(initial_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking length of prod_url\n",
    "len(prod_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening each 226 prod_url and scraping links of all reviews href \n",
    "# open list to save all reviews href\n",
    "rev_cli = []\n",
    "\n",
    "# loop for opening all links stored in prod_url\n",
    "for j in prod_url:\n",
    "    # opening each links of prod_url one by one\n",
    "    driver.get(j)\n",
    "    \n",
    "    try:\n",
    "        # finding links from all webpage by xpath\n",
    "        for k in driver.find_elements_by_xpath(\"//a[@id = 'acrCustomerReviewLink']\"):\n",
    "            rev_cli.append(k.get_attribute('href'))\n",
    "    # if all reviews link is not available, it will continue to next page,so avoiding NoSuchElementException \n",
    "    except NoSuchElementException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length of open list\n",
    "len(rev_cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now opening each links as stored in list above\n",
    "# creating open list for storing all reviews href\n",
    "all_reviews = []\n",
    "\n",
    "for l in rev_cli:\n",
    "    # opening each links as stored in cli_rev open list\n",
    "    driver.get(l)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        # finding links from all webpage by xpath\n",
    "        all_rev = driver.find_element_by_xpath(\"//div[@id = 'reviews-medley-footer']/div[2]/a\")\n",
    "        all_reviews.append(all_rev.get_attribute('href'))\n",
    "        # if all reviews link is not available, it will continue to next page,so avoiding NoSuchElementException \n",
    "    except NoSuchElementException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length of all reviews link\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally opening all_reviews link and scraping ratings and reviews data from all webpages.\n",
    "# creating open list to save scraped data\n",
    "ratings = []\n",
    "reviews = []\n",
    "\n",
    "# opening each link to begin the scaping\n",
    "for m in all_reviews:\n",
    "    driver.get(m)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # loop to open link till page 5 and scraping data of reviews and ratings of products\n",
    "    for n in range(0,5):\n",
    "        for o in driver.find_elements_by_xpath(\"//div[@class = 'a-section review aok-relative']/div/div/div[2]/a[1]\"):\n",
    "            ratings.append(o.get_attribute('title').replace(' out of 5 stars',''))\n",
    "            \n",
    "        for p in driver.find_elements_by_xpath(\"//div[@class = 'a-section review aok-relative']/div/div/div[4]/span/span\"):\n",
    "            reviews.append(p.text)\n",
    "                        \n",
    "        try:\n",
    "            next_page = driver.find_element_by_xpath(\"//ul[@class = 'a-pagination']/li[2]/a\")\n",
    "            driver.get(next_page.get_attribute('href'))\n",
    "        except NoSuchElementException as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>This is got to be best ultrabook which is pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Amazing spects under 60k but complete details ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If graphic processor detail mentioned that sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Read the update at the end.\\nMy review:-\\nGot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Worth Every Penny.\\nThis laptop is amazing.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18259</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The product is simple and does the job very ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18260</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I used the CRV12V2 with my 4G LTE VoLTE router...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18261</th>\n",
       "      <td>5.0</td>\n",
       "      <td>It is very good product, i have problem with p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18262</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Purchased this few days ago because of the fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Awesome product, works with jio fibre router. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ratings                                        Full_review\n",
       "0         4.0  This is got to be best ultrabook which is pric...\n",
       "1         4.0  Amazing spects under 60k but complete details ...\n",
       "2         1.0  If graphic processor detail mentioned that sho...\n",
       "3         1.0   Read the update at the end.\\nMy review:-\\nGot...\n",
       "4         5.0   Worth Every Penny.\\nThis laptop is amazing.\\n...\n",
       "...       ...                                                ...\n",
       "18259     5.0  The product is simple and does the job very ef...\n",
       "18260     5.0  I used the CRV12V2 with my 4G LTE VoLTE router...\n",
       "18261     5.0  It is very good product, i have problem with p...\n",
       "18262     5.0  Purchased this few days ago because of the fre...\n",
       "18263     1.0  Awesome product, works with jio fibre router. ...\n",
       "\n",
       "[18264 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving scraped data into dataframe.\n",
    "search = pd.DataFrame({}) \n",
    "search['Ratings'] = ratings[:18264]\n",
    "search['Full_review'] = reviews[:18264]\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving scraped data in csv format\n",
    "search.to_csv('Ratings_Prediction_Amazon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPING DATA FROM FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating python program to search desired keywords one by one and saving links in open list prod_url\n",
    "\n",
    "# creating open list to save href links of all products \n",
    "\n",
    "prod_url = []\n",
    "\n",
    "# Keywords for search to which we need to scrape the data.\n",
    "keywords = ['Laptops', 'Phones', 'Headphones', 'Smart Watches', 'Cameras', 'Printers', 'Monitors','Home theater', 'Router']\n",
    "\n",
    "# Location of chrome web driver and launching it\n",
    "driver = webdriver.Chrome(\"C:/Flip Robo/Ratings Project/chromedriver.exe\")\n",
    "# Website from which we have to scrape data\n",
    "driver.get(\"https://www.flipkart.com/\") \n",
    "\n",
    "# Closing login pop-up\n",
    "pop_btn = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "\n",
    "# The program will return to present website and will change the search keyword \n",
    "initial_url = driver.current_url\n",
    "\n",
    "# loop for sending keywords to website and scraping href\n",
    "for i in keywords:\n",
    "    # finding search job tab and giving input of desired products as mentioned in keywords\n",
    "    search = driver.find_element_by_xpath(\"//input[@type = 'text']\").send_keys(i)\n",
    "    \n",
    "    \n",
    "    # Clicking on Search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # scraping href of all products\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class = '_1fQZEK' or @class = '_2rpwqI']\"):\n",
    "        prod_url.append(j.get_attribute('href'))\n",
    "        \n",
    "    driver.get(initial_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking length of prod_url\n",
    "len(prod_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening each 296 prod_url and scraping links of all reviews href \n",
    "# open list to save all reviews href\n",
    "rev_cli = []\n",
    "\n",
    "# loop for opening all links stored in prod_url\n",
    "for j in prod_url:\n",
    "    # opening each links of prod_url one by one\n",
    "    driver.get(j)\n",
    "    \n",
    "    try:\n",
    "        # finding links from all webpage by xpath\n",
    "        allrev = driver.find_element_by_xpath(\"//div[@class = 'col JOpGWq']/a\")\n",
    "        ur = allrev.get_attribute('href')\n",
    "        rev_cli.append(ur)\n",
    "    # if all reviews link is not available, it will continue to next page,so avoiding NoSuchElementException \n",
    "    except NoSuchElementException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length of open list\n",
    "len(rev_cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening each url and scraping links of all reviews href \n",
    "# open list to save all reviews href\n",
    "next_cli = []\n",
    "\n",
    "for r in rev_cli:\n",
    "    driver.get(r)\n",
    "    # Extracting href and appending it to a list\n",
    "    for s in driver.find_elements_by_xpath(\"//a[@class = 'ge-49M' or @class = 'ge-49M _2Kfbh8']\"):\n",
    "        next_cli.append(s.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally opening all_reviews link and scraping ratings and reviews data from all webpages.\n",
    "# creating open list to save scraped data\n",
    "ratings_1 = []\n",
    "full_review = []\n",
    "\n",
    "# Loop for opening each href and scrap the data\n",
    "for b in next_cli:\n",
    "    driver.get(b)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # xpath for scraping ratings\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='col']/div/div[1]/div\"):\n",
    "        ratings_1.append(j.text) # appending scraped data in ratings_1 list\n",
    "    \n",
    "    # xpath for scraping full_review\n",
    "    for k in driver.find_elements_by_xpath(\"//div[@class = 't-ZTKy']/div/div\"):\n",
    "        full_review.append(k.text.replace('\\n',\" \")) # appending scraped data in full_review list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>An affordable beast ! Pros: 1. Incredible perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>★</td>\n",
       "      <td>To be honest Pro's 1) RGB keyboard 2)144Hzs wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best laptop in this price segment.. battery is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This laptop is a beast, and a steal for your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Laptop is a masterpiece with stunnig desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>1</td>\n",
       "      <td>Good for baas lovers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>4</td>\n",
       "      <td>Good sound Good bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>5</td>\n",
       "      <td>really nice bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent product nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>3</td>\n",
       "      <td>Superb Very Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ratings                                        Full_review\n",
       "0         4.5  An affordable beast ! Pros: 1. Incredible perf...\n",
       "1           ★  To be honest Pro's 1) RGB keyboard 2)144Hzs wi...\n",
       "2           5  Best laptop in this price segment.. battery is...\n",
       "3           4  This laptop is a beast, and a steal for your m...\n",
       "4           5  The Laptop is a masterpiece with stunnig desig...\n",
       "...       ...                                                ...\n",
       "14995       1                               Good for baas lovers\n",
       "14996       4                               Good sound Good bass\n",
       "14997       5                                   really nice bass\n",
       "14998       5                             excellent product nice\n",
       "14999       3                                   Superb Very Good\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe and saving scraped data into it\n",
    "Reviews = pd.DataFrame({})\n",
    "Reviews['Ratings'] = ratings_1[:15000]\n",
    "Reviews['Full_review'] = full_review[:15000]\n",
    "Reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
